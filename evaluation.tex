%p.11 different frame rate, different device width, different device result
%p.12 tx fps up, distance, exposure
%p.13 iphone/time evaluataion
\section{Evaluation}
\subsection{Experimental Methodology}
 In \autoref{sec:unsync}, we derive the probability of symbol loss and mixed frame. The value is affected by the transmitting frame rate ($T_{f,tx}$), receiving frame rate ($T_{f,rx}$), the read-out time of the camera ($T_r$) and the height of the LED in the received image ($H$). 
 Therefore, we present the experimental results of our unsynchronized VLC system with respect to these factors. The performance metrics we use in the experiments are Packet Reception Rate (PRR) and throughput.

 Each symbol represents 4-bit, and thus we have 16 symbols. 
 The symbol-to-frequency mapping table is generated by.... (formula).
 \autoref{fig:exp1_setup} shows a photo illustrating our setup. 

\subsection{Different Receiving Frame Rate}
First, we want to show the improvement of the decoding performance due to the design of the symbol delimiter, the sequence number, and the parity symbol. 

We alter the receiving frame rate to obtain different tx/rx frame rate ratios from 0.9 to 2 to force unsynchronized transmitter and receiver pairs. That is, we fix the transmitting frame rate at 30 fps, and alter the receiving frame rate from 15 fps to 33 fps. We randomly generate 20 string sequences for each of the three settings: (1) no additional schemes; (2) with only symbol delimiter; and (3) with symbol delimiter, sequence number,  and parity symbol. 
The length of the string sequence is 20 bytes. Note that we also record the videos at random starting time to introduce random phase difference between the transmitter and the receiver.

\autoref{fig:exp1_1} shows the result. When the receiving frame rate and the transmitting frame rate are very close to each other, the first setting has slightly lower PRR, as a number of received image frame will be mixed with two symbols and a portion of them will be demodulated correctly. The other two settings perform similarly. 
However, as the difference between the frame rates increase, the differences in PRRs become very obvious. 

% \begin{figure}[!htb]
%   \centering
%   %\hspace{-5em}
%   \includegraphics[scale=0.075]{fig/exp4_setup.JPG}
%   \caption{Experimental setup photo.}
%   \label{fig:exp1_setup}
% \end{figure}
\begin{figure}[!t]
\centering
  \begin{subfigure}[h]{0.2\textwidth}
  \includegraphics[width=\textwidth]{pic/env_smartphone.JPG}
  \caption{}
  \end{subfigure}
  ~ 
  \begin{subfigure}[h]{0.2\textwidth}
  \includegraphics[width=\textwidth]{pic/env_flea.JPG}
  \caption{}
  \end{subfigure}
\caption{Experimental setup photos with different Receivers: (a) Smartphone (b) Flea3}
\label{fig:exp1_setup}
\end{figure}

The $P_{miss}^ \prime$ and parity symbol ratio are also shown in \autoref{fig:exp1_1}. 
For the third setting, we use $P_{miss}$ to determine the number of parity symbols required to be inserted into a packet. When the receiving frame rate is higher than 19.5 fps, the parity symbol ratio is configured to be 0.25, which means for every three symbols, we add a parity symbol. (It is possible to use a parity symbol rate less than 0.25, following the pink line in the figure. However, we do not discuss such a case here, and will discuss the maximum throughput in \autoref{sec:maxthroughput}.) When the receiving frame rate is between 19.5 and 18 fps, we use a parity symbol ratio of 0.33. When the receiving frame rate is lower than 18 fps, we use a parity symbol ratio of 0.5, which means for every symbol, we transmit twice.
With all 3 schemes, close to 100\% of the packets are received correctly, while the other two settings rapidly lose the ability to receive correct packets when the difference between frame rates increases. The results verify that our design can indeed address the issues caused by unsynchronized transmitter and receiver pair. 

We can see when the receiving frame rate and the transmission frame rate are close, the setting with the symbol delimiter has the highest throughput. As the difference between the frame rates increases, the throughput of the third setting decreases due to the large parity symbol ratio.

\subsection{Varying Inter-frame Interval}
% \begin{figure}[!htb]
%   \centering
%   %\hspace{-2em}
%   \includegraphics[scale=0.25]{fig/exp1_new.eps}
%   \caption{Result under different rx fps.}
%   \label{fig:exp1_1}
% \end{figure}
% \begin{figure}[!htb]
%  % \centering
%   \hspace{-5em}
%   \includegraphics[scale=0.2]{fig/exp2_tr_new.eps}
%   \caption{Pixel width under different smartphone devices.}
%   \label{fig:exp2_tr}
% \end{figure}
% \begin{figure}[!htb]
%  % \centering
%   \hspace{-3em}
%   \includegraphics[scale=0.2]{fig/exp2_refine.eps}
%   \caption{Result under different smartphone devices.}
%   \label{fig:exp2_1}
% \end{figure}

\begin{figure*}[!t]
  %\centering
	\begin{subfigure}[h]{0.3\textwidth}
	  \includegraphics[width=\textwidth]{fig/exp1_new.eps}
	  \caption{}
	  \label{fig:exp1_1}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.35\textwidth}
	  \includegraphics[width=\textwidth]{fig/exp2_tr_new.eps}
	  \caption{}
  	  \label{fig:exp2_tr}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.3\textwidth}
  	  \includegraphics[width=\textwidth]{fig/exp2_refine.eps}
      \caption{}
      \label{fig:exp2_1}
    \end{subfigure}
    \caption{(a) Result under different rx fps. (b) Pixel width under different smartphone devices. (c) Result under different smartphone devices.}
    \label{}
\end{figure*}
According to~\cite{hu2013lightsync}, the receiving frame rate of smartphone cameras exhibit more variability for several reasons. Even when the average frame rate appears steady, the inter-frame interval still varies. 
In order to show the improvement with our new design scheme, we emulate this phenomena with a system with time varying inter-frame interval. 
However, it is hard to alter the receiving frame rate while receiving frames. We therefore use the Flea3 camera with fixed receiving frame rate and alter the transmitting frame rate before transmitting every symbol. 

To simulate the various of transmitting frame rate, we use a truncated normal distribution with the mean value of 36.9458 fps, the standard deviation of 3.94, the min value 31.03 fps and the max value 42.86 fps. The values are based on observations presented in~\cite{hu2013lightsync}, which states that the receiving frame rate fluctuates significantly between 21 to 29 fps for an entirely white foreground, with an average of around 25 fps. 

We again generate 20 random string sequences. The length of each string sequence is 20 bytes. For each transmitting symbol, we randomly pick a number from the truncated normal distribution and transmit the symbol with the generated frame rate.
We use the PointGrey Flea3 camera to record the video at fixed 30 fps. 

% \autoref{tab:norm_result} shows the result of PRR and throughput.
% First, we want to know how frequently mixed frame or lost symbol occur. Thus, we use the average transmitting frame rate 36.9458 fps to calculate the probability. We get $P_{miss}$ = 0.0501 and $P_{mix}$ = 0.5877.
% We can see that with no additional schemes, the PRR is 0 since there are a large number of mixed frames. With only the symbol delimiter setting, the PRR is still 0 because there is a non-zero probability to have missed symbol. With all three schemes, the PRR is 1. It is worth mentioning that we use a lower parity symbol ratio to increase the throughput in this experiment.

% \begin{table}[!htb]
% \centering
% \caption{PRR and Throughput under various inter-frame interval.}
%         %\tabcolsep=1cm
%         %\tabcolsep=0.08cm
%         \begin{tabular}{lcc}
%         \hline Setting & PRR & Throughput\\ 
%         \hline \hline
%         No additional schemes & 0 & 0 bytes per second\\
%         With only symbol delimiter   & 0 & 0 bytes per second \\
%         With symbol delimiter, sequence number and parity symbol ratio 0.125 & 1 & 13.94 bytes per second\\
%         With symbol delimiter, sequence number and parity symbol ratio 0.143 & 1 & 13.68 bytes per second\\
%         With symbol delimiter, sequence number and parity symbol ratio 0.167 & 0.95 & 11.89 bytes per second\\
%         With symbol delimiter, sequence number and parity symbol ratio 0.2 & 1 & 12.31 bytes per second\\
%         %With frame delimiter, sequence number and parity symbol ratio 0.25 & 1 & 12.1134\\
%         %With frame delimiter, sequence number and parity symbol ratio 0.33 & 1 & 10.8664 \\
%         %With frame delimiter, sequence number and parity symbol ratio 0.5 & 1 & 8.4933 \\
%         \hline
%         \end{tabular}
%         \label{tab:norm_result}
% \end{table}

\subsection{Different Smartphone Cameras}
Third, we use several smartphones to compare the decoding performance under the three previous mentioned settings. 
Note that the receiving frame rate of the smartphones are not fixed. Moreover, the read-out time and the resolution of the camera of smartphones are also different. These will affect the probabilities of symbol loss and mixed frame. We again generate 20 random string sequences. The length of each string sequence is 20 bytes. 

As the read-out time of the smartphones are not the same. This means different smartphone cameras may observe the different pixel width (signal period) under the same transmitting frequency. Therefore we use the preamble symbol to calibrate the $T_r$ to minimize the error of the period estimates of all subsequent symbols in the packet. \autoref{fig:exp2_tr} shows the pixel width of different smartphone cameras. %We use the pixel width of Flea3 as the base (the X-axis), and the Y-axis represents the observed pixel width. Generally, the pixel width grows linearly. What we need to do is to calibrate the lines to fit the top black line. 

\autoref{fig:exp2_1} shows the result. The X-axis shows different smartphone cameras. We first calculate $P_{miss}$ by averaging the frame rate of each smartphone camera. The obtained values are all zeros since the average fps are close to 30. However, we can see that $P_{mix}$ is high.

For the first setting, we can see the PRR is about 50\% to 60\%. The result also shows that when the $P_{mix}$ is higher, PRR of the first setting gets lower.
For the second and third settings, basically that PRRs are similar since no symbol miss take place. However, we can see that most of the devices show that the third setting gets lower PRR then the second setting. Some of the devices also show that the third setting gets lower PRR then the first setting. Furthermore, the overall PRR is on the low side. We think the problem is due to decoding error. Because of the instability of the smartphone cameras, if the received pixel width of the preamble has slightly error, it will affect the third setting very much because the pixel width range we use in the third setting is larger than the first two settings. The yellow and cyan bars in the figure represent the result ignoring the preamble error. We can see that PRRs of the second and third settings are very close to 1. (From this experiment, we can see that the second setting is good enough for the communications in spite of using different smartphones. However, the current setting uses regular video recording mode rather than preview mode, and the receiver is very close to the transmitter. That is, the are many factors not considered yet and these will be discussed in the following subsection.) Therefore, the error are mainly caused by the preamble calibration rather than the unsynchronized issues. To address the preamble calibration error, we can transmit more preamble symbols at the beginning of the packet and average them when receiving these symbols. This solution can minimize the error caused by calibration but will increase the packet overhead at the same time.

We also found that the front-facing camera of HTC NewOne cannot decode any packet in all three settings. We observed that the behavior of the camera records every frame twice. Even when we extract half of the frames in the video, it is still not able to decode.
Therefore, we think the problem is not related to our decoding method.
Our results indicate that, the second setting can obtain the maximum throughput in most settings.

% \begin{figure}[!t] 
%  %\centering
%   \hspace{-2em}
%   \includegraphics[scale=0.2]{fig/exp3_iphone5s_new_modify.eps}
%   \caption{Result of iPhone5s under different transmitting frame rate.}
%   \label{fig:exp3_3}
% \end{figure}
% \begin{figure}[!htb] 
%  %\centering
%   %\hspace{-3em}
%   \includegraphics[scale=0.25]{fig/exp4_iphone5s_new.eps}
%   \caption{Result of iPhone5s under different LED height.}
%   \label{fig:exp4_2}
% \end{figure}
\begin{figure*}[!t]
  %\centering
	\begin{subfigure}[h]{0.3\textwidth}
	  \includegraphics[width=\textwidth]{fig/exp3_iphone5s_new_modify.eps}
	  \caption{}
	  \label{fig:exp3_3}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.3\textwidth}
	  \includegraphics[width=\textwidth]{fig/exp4_iphone5s_new.eps}
	  \caption{}
  	  \label{fig:exp4_2}
	\end{subfigure}
	% ~
	% \begin{subfigure}[h]{0.3\textwidth}
 %  	  \includegraphics[width=\textwidth]{}
 %      \caption{}
 %      \label{}
 %    \end{subfigure}
    \caption{(a) Result of iPhone5s under different transmitting frame rate. (b) Result of iPhone5s under different LED height.}
    \label{}
\end{figure*}

\subsection{Max Throughput under Different Transmitting Frame Rate}
\label{sec:maxthroughput}
In this experiment, we fix the receiving frame rate and gradually increase the transmitting frame rate to determine the maximum achievable throughput, and when we can achieve it. As the transmitting frame rate gets higher, $P_{miss}$ increases, and thus we need to use a higher parity ratio to compensate. However, the transmitted data rate also increases. We would like to investigate at what transmitting frame rate the maximum throughput can be obtained.
In this experiment, we increase the transmitting frame rate from 33 fps to 90 fps. The iPhone5s is at 29.98 fps on average. We only use the third setting in this experiment.

\autoref{fig:exp3_3} shows the result of iPhone5s. We can see $P_{miss}$ grows from 0 to over 0.5, and the parity symbol ratio we select is just above the $P_{miss}$ line. Note that the parity symbol ratio we use is at most 0.5. Because we think the overhead costs too much if a symbol is transmitted over two times. We can see that PRR is more than 90\% until the transmitting frame rate reaches 81 fps. 
Theoretically, the upper limit of the transmitting frame rate is bounded by the case of having two symbols in a received frame, which is given by 
\begin{equation}
%Max\_tx\_fps = \frac{1}{(H - 2*FD\_width) T_r} \qquad \textrm{.}
FPS_{Max,tx}= \frac{1}{(H - 2H_{SD}) T_r} \qquad \textrm{.}
\end{equation}

The max transmitting frame rate for iPhone5s is about 53.39 fps, two symbols in a received frame rarely happens. At that time the parity symbol can be used to recover data. Therefore, we can get even higher transmitting frame rate in practice.

The PRR is above 80\% until reaches the transmitting frame rate of 81 fps. We can see that the iPhone5s can have high PRR when the transmitting frame rate is lower than 81 fps.
We can see the throughput line grows with a trend - at the time the parity symbol ratio increases, the throughput would stay the same or drop a little. The maximum throughput of iPhone5s happens at 51 fps, and the throughput has increased by 1.28 times and reaches 19.25 bytes per second.



\subsection{Different LED Size in the Image}

So far, we have considered the factors of transmitting frame rate, receiving frame rate and the read-out time of the camera. There is one more factor - the image height, which may affect $P_{miss}$ and $P_{mix}$. Thus, in this experiment, we try to change the height of the LED in the received image.
There are two methods to do so, one is use the same setting with the previous experiments (which let the LED light full of the image) and simply crop the image into the height we want. The other is to move the receiver from near to far. We choose the latter method. When we get far away from the LED, although the intensity of light and the detection of the LED boundary may also cause some decoding error, we think the latter method exhibits a setting closer to the real situation. 

We use the height (pixel) of the LED in the received image as our X-axis in \autoref{fig:exp4_2}. The transmitting frame rate is configured to 30 fps. The iPhone5s is at 29.98 fps on average. We only use the third setting in this experiment. As to how far away we can reach, it depends on the sensor size of the camera, the focal length of the lens, and the size of the LED. 
%\autoref{tab:camera_para} lists the parameters of the cameras we use in the experiment. 
If we know all the parameters of the camera and lens, the distance from the LED to the camera is proportional to:

\begin{multline}
	distance (cm) \propto \\
	\frac{focal\_length (mm) * LED\_height (mm) * image\_height (pixels)}{LED\_height (pixels) * sensor\_height (mm)} 
	\qquad \textrm{.}
\end{multline}

We also provide a comparison in ~\autoref{tab:comp_dis}, showing the relationship between the distance and LED height in the received image of Flea3 and iPhone5s. The real LED height we use in the experiment is 4.7 cm. In the table, we can see if the LED object in the image is around 500 pixel, the distance from the object to the Flea3 can be half a meter, while the iPhone5s can only be 15 centimeters. However, if the size of the image area illuminated by the light becomes bigger, (For transmitter-end, we can simply use the lampshade to enlarge the light range. For receiver-end, we can use the reflected light from surrounding areas, such as a wall.) we can further lift the distance limitation. 

% \begin{table}[!htb]
% \centering
% \caption{Camera parameters of Flea3 and iPhone5s.}
%   %large
%   \begin{tabular}{lcc}
%   \hline Parameters & Flea3 & iPhone5s \\
%   \hline 
%   \hline Camera Sensor Format & 1/2.5" & 1/3" \\
%   \hline Image Height(pixel) & 1080 & 1080\\
%   \hline Pixel Size($\mu$m) & 1.55 & 1.5 \\
%   \hline Focal Length(mm) & 16 & 4.21 \\
%   \end{tabular}
%   \label{tab:camera_para} 
% \end{table}

Note that we still have a limitation that the transmitting symbol in the received image need to occupy at least two black-white strips for detecting the signal period. For example, symbol with the largest strip width received from Flea3 is 58 pixel, so the image area need to have at least a height of 232 pixel. It means if the height of LED in the received image is smaller than 232 pixel, then the received symbol cannot be decoded.

\begin{table*}[!t]
\centering
\caption{Distance table of Flea3 and iPhone5s.}
  %\large
  \hspace{-2em}
  \tabcolsep=0.11cm
  \begin{tabular}{lcccccccccc}
  \hline LED height in image (pixel) & 1000 & 900 & 800 & 700 & 600 & 500 & 400 & 300 & 200 & 100 \\
  \hline 
  \hline Flea3 distance (cm) & 48.5 & 53.9 &60.6& 69.3 &80.9& 97.0& 121.3 &161.7 &242.6 &485.2\\
  \hline iPhone5s distance (cm) & 7.75& 8.61& 9.68& 11.07& 12.91& 15.49& 19.36& 25.82& 38.73& 77.46 \\
  \end{tabular}
  \label{tab:comp_dis} 
\end{table*}


\autoref{fig:exp4_2} shows the results of iPhone5s, due to its longer read-out time and shorter time gap between receiving frames, $P_{miss}$ is relatively smaller. Thus, when the LED has a height of around 300 pixel in the image, the transmission can still be received. The PRR is about 75\%.
 
 We can see the iPhone5s can achieve 9.5 $\sim$ 12 bytes per second when the LED height is above 500 pixel. As the LED object in the image becomes small, the throughput decreases significantly; due to both a smaller PRR and a large parity symbol ratio.


\subsection{Time Measurement of Real-time iOS Decoder App}
\label{sec:ios_eval}
%(needed if preview mode is not equal to video mode)
%(Figure: x:tx/rx ratio, y:PRR, color line:mode, for iphone5s app)

We have shown that our system work well for cameras on mobile devices. In this subsection, we want to know whether the decoding process can be executed in real-time with smartphone processors, we also implement an iOS decoding app, running on iPhone5s.

We use the AV Foundation framework, which provides an Objective-C interface for managing and playing audio-visual media in iOS applications. The application uses multiple threads. One thread obtains the frame buffers captured by the camera, and put the image buffers into the queue to be processed. The other thread takes the frame at the front of the queue and decodes it in real time without saving the image. Decoding results for the frame will be shown on the phone screen. \autoref{fig:ios_gui} shows the GUI of the iOS application; one can first touch the screen where the light locates to lock the focus and the exposure time. After pressing start button, the application will start to detect the LED location and decode the message.

% \begin{figure}[!t]
%   \centering
%   \includegraphics[scale=0.25]{fig/ios_gui3.png}
%   \caption{iOS application GUI}
%   \label{fig:ios_gui}
% \end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.075]{pic/ios_using.JPG}
  \caption{iOS application GUI}
  \label{fig:ios_gui}
\end{figure}

%\subsection{Optimizations for real-time processing}
Since we have set the FrameDuration of the captured video data output (for example, we set the video to 30 fps), we need to make sure that we have enough time to process a frame within one FrameDuration.

To optimize the processing time, we first add the optimization level of Apple LLVM 5.1 compiler as "-Os", generating a faster and smaller executable.

Next, we time individual operations to assess their computational complexity. We found that the majority of the processing time happens when we try to get the frame buffer and convert it to the UIImage, then convert the UIImage to the OpenCV Mat type. This step takes about 0.04 sec, which is greater than a frame duration. To reduce the time, we directly get the pixel base address and put it into Mat. Surprisingly, the time reduces to 0.00003 sec.

Moreover, we found that when we convert RGB to gray scale, it also costs about 0.02 sec. To reduce the time, we change the video output pixel format to YUV color space, and use the luminance(Y) channel as the gray scale value. As a result, we save 0.02 sec.

After a series of optimizing steps, the total processing time of a frame is about 0.007 sec which is less than $1/30$ sec. Since our current implementation already meets the real-time target, we do not explore further optimizations, even though some operations could be sped up more.
 \autoref{tab:ios_time} lists the time taken for each operation.

\begin{table}[!t]
\centering
\caption{Processing time breakdown in ms}
        %\tabcolsep=1cm
        %\large
        \begin{tabular}{lc}
        \hline Operation & time(ms) \\ \hline \hline
        Image Buffer to Mat Conversion & 0.03 \\
        Symbol Delimiter Detection & 3.25 \\
        YIN Period Detection & 3.74 \\
        GUI Text Output & 0.04 \\
        \hline
        Total & 7.06 \\
        \hline
        \end{tabular}
        \label{tab:ios_time}
\end{table}